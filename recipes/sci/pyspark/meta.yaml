package:
  name: pyspark
  version: 1.6.0

source:
  url: http://d3kbcqa49mib13.cloudfront.net/spark-1.6.0-bin-hadoop2.6.tgz
  fn: spark-1.6.0-bin-hadoop2.6.tgz
#  patches:
   # List any patch files here
   # - fix.patch

# build:
  #preserve_egg_dir: True
  #entry_points:
    # Put any entry points (scripts to be generated automatically) here. The
    # syntax is module:function.  For example
    #
    # - scikit-learn = scikit-learn:main
    #
    # Would create an entry point called scikit-learn that calls scikit-learn.main()


  # If this is a new build for the same version, increment the build
  # number. If you do not include this key, it defaults to 0.
  # number: 1

requirements:
  build:
    - python
    - numpy
    - scipy
    - setuptools

  run:
    - python
    - numpy
    - scipy



test:
  # Python imports
  imports:
    - pyspark

  #commands:
    # You can put test commands to be run here.  Use this to test that the
    # entry points work.


  # You can also put a file called run_test.py in the recipe that will be run
  # at test time.

  # requires:
    # Put any additional test requirements here.  For example
    # - nose

about:
  home: https://spark.apache.org
  license: Apache License
  summary: 'Apache Spark'

# See
# http://docs.continuum.io/conda/build.html for
# more information about meta.yaml
